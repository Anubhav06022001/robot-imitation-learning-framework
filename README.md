 # ðŸ¤– Robot Imitation Learning Framework

A modular framework for **imitation learning in robotics**, currently focused on **dynamic brachiation control** using MuJoCo.  
The system is task-agnostic â€” provide expert demonstrations, and it learns the behavior.

---
## Installation
```bash
conda env create -f environment.yml
conda activate robot-il
```

## ðŸ”¥ Implemented & Upcoming Methods

Currently supported:
- GAIL â€“ Generative Adversarial Imitation Learning âœ”
- AIRL â€“ Adversarial IRL âœ” 

Upcoming algorithms (will be uploaded soon):
- DAgger â€“ Dataset Aggregation  
- Residual Diffusion Imitation Policies  
- GAIL-ES â€“ Evolution Strategies + GAIL  

This repository is actively evolving to benchmark **multiple IL algorithms** under the same environment and training structure.

---

## ðŸ“ Project Structure

```text

robot-imitation-learning-framework/
â”œâ”€ il_lib/
â”‚  â”œâ”€ envs/
â”‚  â”‚  â””â”€ brachiation_env.py
â”‚  â”‚
â”‚  â”œâ”€ datasets/
â”‚  â”‚  â”œâ”€ expert_dataset.py        # generic (s,a) loader
â”‚  â”‚  â””â”€ airl_dataset.py          # (s,a,s',done) version
â”‚  â”‚
â”‚  â”œâ”€ common/
â”‚  â”‚  â”œâ”€ replay.py
â”‚  â”‚  â”œâ”€ utils.py
â”‚  â”‚  â””â”€ trajectory_logger.py
â”‚  â”‚
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ policy_gaussian.py       # PolicyNetwork
â”‚  â”‚  â”œâ”€ bc.py                    # BC_MLP
â”‚  â”‚  â”œâ”€ reward_net.py            # AIRL RewardNet
â”‚  â”‚  â”œâ”€ value_net.py             # AIRL ValueNet
â”‚  â”‚  â””â”€ diffusion_denoiser.py    # DiffusionDenoiser
â”‚  â”‚
â”‚  â”œâ”€ gail/
â”‚  â”‚  â””â”€ gail_trainer.py
â”‚  â”‚
â”‚  â”œâ”€ airl/
â”‚  â”‚  â””â”€ airl_trainer.py
â”‚  â”‚
â”‚  â”œâ”€ diffusion/
â”‚  â”‚  â”œâ”€ policy.py                # DiffusionPolicy
â”‚  â”‚  â”œâ”€ sampling.py              # DDIM / sampling
â”‚  â”‚  â”œâ”€ noise_schedule.py        # betas, alphas
â”‚  â”‚  â””â”€ trainer.py               # diffusion training
â”‚  â”‚
â”‚  â””â”€ dagger/
â”‚     â””â”€ dagger_trainer.py        # (later)
â”‚
â”œâ”€ scripts/
â”‚  â”œâ”€ train_gail.py
â”‚  â”œâ”€ train_airl.py
â”‚  â””â”€ train_diffusion.py
â”‚
â”œâ”€ configs/
â”‚  â”œâ”€ gail.yaml
â”‚  â”œâ”€ airl.yaml
â”‚  â””â”€ diffusion.yaml
â”‚
â”œâ”€ data/
â”‚  â””â”€ .gitkeep
â”‚
â”œâ”€ tests/
â”‚  â””â”€ ...
â”‚
â””â”€ README.md






robot-imitation-learning-framework/
â”œâ”€ il_lib/
â”‚  â”œâ”€ envs.py                 # MuJoCo-based environments (BrachiationEnv)
â”‚  â”œâ”€ models.py               # PolicyNetwork (Gaussian), Discriminator
â”‚  â”œâ”€ expert_dataset.py       # Expert dataset loader (CSV -> tensors)
â”‚  â”œâ”€ replay.py               # Replay buffer for policy (s,a) samples
â”‚  â”œâ”€ utils.py                # Common utilities (logging, etc.)
â”‚  â”œâ”€ losses.py               # IL-specific loss helpers (TODO)
â”‚  â””â”€ trainer/
â”‚     â”œâ”€ gail_trainer.py      # GAIL training loop (current core)
â”‚     â”œâ”€ policy_updater.py    # PPO/actor-critic helpers (TODO)
â”‚     â””â”€ value_updater.py     # Value learning (TODO)
â”‚
â”œâ”€ scripts/
â”‚  â”œâ”€ train_gail.py           # Main training entrypoint
â”‚  â””â”€ evaluate_policy.py      # Policy rollout & diagnostics (TODO)
â”‚
â”œâ”€ configs/
â”‚  â””â”€ default.yaml            # Hyperparameter configs (TODO)
â”‚
â”œâ”€ tests/
â”‚  â”œâ”€ test_env.py             # Env sanity tests
â”‚  â”œâ”€ test_models.py          # Forward pass tests
â”‚  â””â”€ test_trainer_sanity.py  # Sanity check (TODO)
â”‚
â”œâ”€ data/
â”‚  â””â”€ .gitkeep                # Place expert data here
â”‚
â””â”€ README.md
```

## ðŸ“Œ Expert Demonstration Format

Add your expert demonstrations to:  
```bash
`data/expert_data.csv`
```

| Column  | Description                         |
|---------|-------------------------------------|
| time    | simulation timestep                 |
| theta1  | shoulder joint angle                |
| theta2  | elbow joint angle                   |
| dtheta1 | shoulder joint angular velocity     |
| dtheta2 | elbow joint angular velocity        |
| control | expert torque                       |

The policy aims to match the expert **occupancy distribution** in stateâ€“action space.

---

## ðŸš€ Run Training (GAIL)

Install dependencies:

```bash
pip install torch mujoco gymnasium pandas numpy
```

Run training:

```bash
python scripts/train_gail.py
```

Checkpoints will be saved to:

```bash
checkpoints/policy_gail.pth
checkpoints/discriminator_gail.pth
```

ðŸ§  Why Brachiation?

Brachiation is:

underactuated

highly dynamic

contact-rich

requires precise swing timing

These properties make it an ideal benchmark for adversarial imitation learning and learned control in challenging robotic locomotion tasks.

Our goal is to build a reproducible and extendable research platform for state-of-the-art IL in robotics.

ðŸ‘¤ Author

Anubhav Tripathi

ðŸ™Œ Contributions

Open for issues, pull requests, and extensions!
If you build an IL method on this framework, please share! ðŸš€





---


